{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P2Enjoy/ia-web3.fr/blob/main/SpeedWatch_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJcKAw0ibDeF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install all requirements (run once) ~~(GOOD)\n",
        "\n",
        "# System\n",
        "import locale, os\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install --upgrade transformers==4.31.0 torch accelerate==0.21.0 bitsandbytes==0.41.0 tensorflow==2.13.0 onnxruntime-gpu==1.15.1\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Application for ponctuation\n",
        "!pip install deepmultilingualpunctuation\n",
        "!pip install scikit-learn\n",
        "\n",
        "!pip install pytorch-crf==0.7.2 youtube_transcript_api==0.6.1 langchain sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube processing"
      ],
      "metadata": {
        "id": "O6nVxgaEOiTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Put the link of the video to summarize:\n",
        "video_link = \"https://www.youtube.com/watch?v=jQL0ZeHtXFc\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v6lO0flKOz2N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0S4YI0B-dL41",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Extract the youtube video transcript\n",
        "\n",
        "def YTVideoToText(video_link):\n",
        "  # installing & importing libraries\n",
        "  from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "  # fetching video transcript\n",
        "  video_id = video_link.split(\"=\")[1]\n",
        "  transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "  # iterating throughout and adding all text together (recovering ponctuations)\n",
        "  flat_transcript = \"\"\n",
        "  for tu in transcript:\n",
        "      flat_transcript += ' ' + tu['text']\n",
        "\n",
        "  return flat_transcript\n",
        "\n",
        "video_full_text = YTVideoToText(video_link)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restore the ponctuation\n",
        "\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "\n",
        "model = PunctuationModel()\n",
        "video_restored_text = model.restore_punctuation(video_full_text)"
      ],
      "metadata": {
        "id": "wevJ3xXdtvZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "004c6ed5-b3c8-4c56-db74-50b4b367e157"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "nICKbEnxOpmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x66g0wqsddPr"
      },
      "outputs": [],
      "source": [
        "#@title Load the model into memory\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"facebook/bart-large-cnn\" #@param {type:\"string\"}\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "summarizeryt = pipeline('summarization', model= model_id, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Join sentences in paragraphs of less the specified token size\n",
        "summarize_token_width = 1000 #@param {type:\"slider\", min:250, max:2048, step:250}\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer, separator=f'\\. ', chunk_size=summarize_token_width, chunk_overlap=0\n",
        ")"
      ],
      "metadata": {
        "id": "PiFJaGl9NwgL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_sum(sentences):\n",
        "  # summarize text\n",
        "  summarized_text = ''\n",
        "  for sentence in sentences:\n",
        "      out = summarizeryt(sentence, min_length=30, do_sample=False)\n",
        "      out = out[0]\n",
        "      out = out['summary_text']\n",
        "      summarized_text += f'.\\n{out}'\n",
        "\n",
        "  # returning summary\n",
        "  return summarized_text;\n",
        "\n",
        "video_summarized_text = do_sum(splitter.split_text(video_restored_text))"
      ],
      "metadata": {
        "id": "PM6h3Ig_W7ml"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aOagAB7AhheV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "07794152-08ba-4e03-c9c1-2a6b39a89a0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.\\nPart three of how to instruct, fine tune your large language model in a GPT language suit model. In this part of the series, we look at the Stanford alpaca prompt, the Stanford GPT prompt. The prompt asks users to come up with 20 diverse task instruction in English..\\nStanford alpaca prompt takes two beautiful steps. The original human way was explain human behavior, and now it\\'s a much nicer way. This is the power of gbt4 or judge gbt, I don\\'t know whatever..\\nJson was exactly what I, what we have in a second resident here, generate instruction- sorry, this one\\\\. so here we go in and what I want to show you here we start Channel 8: instruction following data, our instruction, our self instruction, generated data for fine tuning. So first we tokenize all the seed instruction. Then we go here to open AI to the utilities by openai, and open AI will create, as I just showed you here, some diverse tasks..\\nThe code implementation of the Stanford alpaca generator instruction, a python file, is shown here. The code implementation shows how to generate the prompt to generate now from open AI, exactly like I showed you here. You can filter out certain images that you do not like..\\nWe have a model we have from the hugging face Transformer, an auto model 4 causal language model from a pre-trained hugging face model, and there we give it the model name and we\\'re on path. Then we model any tokenizer where the tokenizers have been pre- trained. Then you just say trainertrain, as always in my last 100 videos, this is not a command to fine tune your llm. So the only question left is now: here, our data module..\\n\"Why not let chat GPT work for us in explaining a certain piece of code? so let\\'s see, cat item\\\\. is this really the last one? no, where am I getting item? yeah, it\\'s the lastOne, beautiful\\\\. so you see, you do not need me anymore because I just ask here: chat G PT explain this class of supervised data set where we input our data set\" \"We have sources and we have targets and warning, and then we have data deck, pre-process sources, Pockets, tokenizer, and from the dictionary we have here our input IDs and our label\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#@title Show the summarized text of the video\n",
        "video_summarized_text"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}