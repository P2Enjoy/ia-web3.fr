{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P2Enjoy/ia-web3.fr/blob/main/SpeedWatch_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJcKAw0ibDeF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install all requirements (run once)\n",
        "\n",
        "# System\n",
        "import locale, os\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install --upgrade transformers==4.31.0 torch accelerate==0.21.0 bitsandbytes==0.41.0 tensorflow==2.13.0 onnxruntime-gpu==1.15.1\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Application for ponctuation\n",
        "!pip install deepmultilingualpunctuation\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "\n",
        "!pip install pytorch-crf==0.7.2 youtube_transcript_api==0.6.1 langchain sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube processing"
      ],
      "metadata": {
        "id": "O6nVxgaEOiTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Put the link of the video to summarize:\n",
        "video_link = \"https://www.youtube.com/watch?v=9JRl-3mN4eo&pp=ygUIYXJpYW5lIDU%3D\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v6lO0flKOz2N"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract the youtube video transcript\n",
        "\n",
        "def YTVideoToText(video_link):\n",
        "  # installing & importing libraries\n",
        "  from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "  # fetching video transcript\n",
        "  video_id = video_link.split(\"=\")[1]\n",
        "  transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "  # iterating throughout and adding all text together (recovering ponctuations)\n",
        "  flat_transcript = \"\"\n",
        "  for tu in transcript:\n",
        "      flat_transcript += ' ' + tu['text']\n",
        "\n",
        "  return flat_transcript\n",
        "\n",
        "video_full_text = YTVideoToText(video_link)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-3J6ghADMV8R"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S4YI0B-dL41",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Extract tyhe video transcript for all youtube video\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def YTVideoToText(video_link):\n",
        "    # Extract video ID from the video_link\n",
        "    parsed_url = urlparse(video_link)\n",
        "    video_id = None\n",
        "    if parsed_url.netloc == 'www.youtube.com':\n",
        "        video_id = parse_qs(parsed_url.query).get('v', [None])[0]\n",
        "    elif parsed_url.netloc == 'youtu.be':\n",
        "        video_id = parsed_url.path.split('/')[1]\n",
        "\n",
        "    if not video_id:\n",
        "        raise ValueError(\"Invalid YouTube video link or missing video ID\")\n",
        "\n",
        "    # Fetching video transcript\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Error fetching transcript: \" + str(e))\n",
        "\n",
        "    # Iterating throughout and adding all text together (recovering punctuations)\n",
        "    flat_transcript = \"\"\n",
        "    for tu in transcript:\n",
        "        flat_transcript += ' ' + tu['text']\n",
        "\n",
        "    return flat_transcript\n",
        "\n",
        "try:\n",
        "    video_full_text = YTVideoToText(video_link)\n",
        "    print(video_full_text)\n",
        "except ValueError as ve:\n",
        "    print(ve)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restore the ponctuation\n",
        "\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "\n",
        "model = PunctuationModel()\n",
        "video_restored_text = model.restore_punctuation(video_full_text)"
      ],
      "metadata": {
        "id": "wevJ3xXdtvZi",
        "cellView": "form"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "nICKbEnxOpmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "x66g0wqsddPr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load the model into memory\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"facebook/bart-large-cnn\" #@param {type:\"string\"}\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#summarizeryt = pipeline('summarization', model= model_id, device=0)\n",
        "summarizeryt = pipeline('summarization', model=model_id, device=-1)  # Utiliser le CPU (-1) au lieu du GPU (0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Join sentences in paragraphs of less the specified token size\n",
        "summarize_token_width = 500 #@param {type:\"slider\", min:250, max:2048, step:250}\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer, separator=f'\\. ', chunk_size=summarize_token_width, chunk_overlap=0\n",
        ")"
      ],
      "metadata": {
        "id": "PiFJaGl9NwgL",
        "cellView": "form"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extrac the summary\n",
        "def do_sum(sentences):\n",
        "  # summarize text\n",
        "  summarized_text = ''\n",
        "  for sentence in sentences:\n",
        "      out = summarizeryt(sentence, min_length=30, do_sample=False)\n",
        "      out = out[0]\n",
        "      out = out['summary_text']\n",
        "      summarized_text += f'.\\n{out}'\n",
        "\n",
        "  # returning summary\n",
        "  return summarized_text;\n",
        "\n",
        "video_summarized_text = do_sum(splitter.split_text(video_restored_text))"
      ],
      "metadata": {
        "id": "PM6h3Ig_W7ml",
        "cellView": "form"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOagAB7AhheV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Show the summarized text of the video\n",
        "video_summarized_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title reformulation du resumé\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Télécharger le modèle 'punkt' et les données WordNet si nécessaire\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Fonction pour obtenir les synonymes d'un mot en utilisant la lemmatisation\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "# Fonction pour reformuler le texte en utilisant NLTK\n",
        "def reformuler_texte(texte):\n",
        "    # Tokenization des phrases\n",
        "    phrases = sent_tokenize(texte)\n",
        "\n",
        "    # Réformulation des phrases\n",
        "    phrases_reformulees = []\n",
        "    for phrase in phrases:\n",
        "        # Tokenization des mots dans la phrase\n",
        "        mots = word_tokenize(phrase)\n",
        "\n",
        "        # Réformulation de chaque mot en utilisant un synonyme aléatoire\n",
        "        mots_reformules = []\n",
        "        for mot in mots:\n",
        "            # Vérifier si le mot est un mot de ponctuation\n",
        "            if mot in string.punctuation:\n",
        "                mots_reformules.append(mot)  # Conserver les caractères de ponctuation inchangés\n",
        "            else:\n",
        "                synonyms = get_synonyms(mot)\n",
        "                if synonyms:\n",
        "                    mot_reformule = random.choice(synonyms)\n",
        "                else:\n",
        "                    mot_reformule = mot  # Conserver le mot inchangé s'il n'y a pas de synonyme\n",
        "                mots_reformules.append(mot_reformule)\n",
        "\n",
        "        # Reconstitution de la phrase réformulée\n",
        "        phrase_reformulee = ' '.join(mots_reformules)\n",
        "        phrases_reformulees.append(phrase_reformulee)\n",
        "\n",
        "    # Concaténer les phrases pour former le texte réformulé\n",
        "    texte_reformule = ' '.join(phrases_reformulees)\n",
        "\n",
        "    return texte_reformule\n",
        "\n",
        "# Réformulation du résumé original\n",
        "resume_reformule = reformuler_texte(video_summarized_text)\n",
        "print(resume_reformule)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pA-taDx18XcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNL4eKLSAL9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}